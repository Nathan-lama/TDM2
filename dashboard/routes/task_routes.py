from flask import Blueprint, render_template, request, redirect, url_for, flash
from config import db, orchestrator_client
from image_urls_generator import get_wikidata_themed_images, get_themed_queries
import uuid
import datetime
import time
import os
import logging

# Configurer le logger
debug_logger = logging.getLogger('debug')
debug_logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
handler.setFormatter(logging.Formatter('DEBUG: %(message)s'))
debug_logger.addHandler(handler)

task_routes = Blueprint('task', __name__, url_prefix='/tasks')

@task_routes.route('/create', methods=['GET', 'POST'])
def create_task():
    """Cr√©e une nouvelle t√¢che"""
    if request.method == 'POST':
        task_type = request.form.get('task_type')
        
        if task_type == 'download':
            # Cr√©er une t√¢che de t√©l√©chargement
            urls = request.form.get('urls', '').strip().split('\n')
            urls = [url.strip() for url in urls if url.strip()]
            
            if not urls:
                flash("Veuillez fournir au moins une URL", "warning")
                return redirect(url_for('task.create_task'))
            
            # Diviser les URLs entre les workers de mani√®re √©quilibr√©e
            
            worker1_urls = urls[::2]  # Indices pairs
            worker2_urls = urls[1::2]  # Indices impairs
            
            if worker1_urls:
                db.download_tasks.insert_one({
                    "_id": str(uuid.uuid4()),
                    "worker_id": 1,
                    "urls": worker1_urls,
                    "status": "pending",
                    "created_at": datetime.datetime.now()
                })
            
            if worker2_urls:
                db.download_tasks.insert_one({
                    "_id": str(uuid.uuid4()),
                    "worker_id": 2,
                    "urls": worker2_urls,
                    "status": "pending",
                    "created_at": datetime.datetime.now()
                })
            
            flash(f"{len(urls)} URLs ajout√©es pour t√©l√©chargement", "success")
            
        elif task_type == 'tagging':
            # ...existing code for tagging task...
            pass
            
        elif task_type == 'recommendation':
            # ...existing code for recommendation task...
            pass
        
        return redirect(url_for('main.index'))
    
    return render_template('create_task.html')

@task_routes.route('/batch_download', methods=['GET', 'POST'])
def batch_download():
    """T√©l√©charge un lot d'images th√©matiques depuis WikiData"""
    if request.method == 'POST':
        # R√©cup√©rer les param√®tres du formulaire
        theme = request.form.get('theme', 'nature')
        count = int(request.form.get('count', 50))
        auto_scale = request.form.get('auto_scale', 'yes') == 'yes'  # Activer par d√©faut
        
        # Option pour nettoyer la base de donn√©es
        clean_db = request.form.get('clean_db') == 'yes'
        
        if clean_db:
            # Cr√©er une t√¢che de nettoyage explicite
            clean_task_id = str(uuid.uuid4())
            db.download_tasks.insert_one({
                "_id": clean_task_id,
                "worker_id": 1,
                "action": "clean",
                "clean_all": True,  # Flag explicite pour tout nettoyer
                "status": "pending", 
                "created_at": time.time(),
                "priority": "high"
            })
            
            # Effectuer le nettoyage en direct dans MongoDB
            try:
                # Compter avant suppression
                image_count = db.images.count_documents({})
                
                # Supprimer les collections principales
                db.images.delete_many({})
                db.tags.delete_many({})
                db.user_likes.delete_many({})
                db.recommendations.delete_many({})
                
                flash(f"Base de donn√©es nettoy√©e: {image_count} images supprim√©es", "success")
            except Exception as e:
                flash(f"Erreur lors du nettoyage de la base de donn√©es: {str(e)}", "danger")
        
        # R√©cup√©rer des images th√©matiques depuis WikiData
        urls = get_wikidata_themed_images(theme, count)
        
        if not urls:
            flash(f"Aucune image trouv√©e pour le th√®me '{theme}'", "warning")
            return redirect(url_for('task.batch_download'))
        
        # Afficher le nombre r√©el d'URLs trouv√©es
        if len(urls) < count:
            flash(f"Seulement {len(urls)} images trouv√©es sur les {count} demand√©es", "info")
        
        # D√©finir le nombre d'images par worker de mani√®re dynamique
        IMAGES_PER_WORKER = 10  # Id√©alement 10 images par worker
        
        # Calculer le nombre optimal de workers en fonction du nombre d'images
        optimal_worker_count = max(1, min(5, (len(urls) + IMAGES_PER_WORKER - 1) // IMAGES_PER_WORKER))
        
        # Utiliser un seul worker pour moins de 10 images
        if len(urls) <= IMAGES_PER_WORKER:
            worker_count = 1
        else:
            worker_count = optimal_worker_count
        
        # Afficher le raisonnement de scaling
        scaling_explanation = f"Autoscaling: {len(urls)} images √∑ {IMAGES_PER_WORKER} images par worker = {worker_count} worker{'s' if worker_count > 1 else ''}"
        flash(scaling_explanation, "info")
        
        # Effectuer l'auto-scaling
        if auto_scale:
            try:
                success, message = orchestrator_client.scale_service("image_downloader", worker_count)
                if success:
                    flash(f"Service image_downloader scaled √† {worker_count} instance{'s' if worker_count > 1 else ''}", "success")
                else:
                    flash(f"√âchec du scaling automatique: {message}", "danger")
            except Exception as e:
                flash(f"Erreur lors du scaling: {str(e)}", "danger")
        
        # Distribution optimis√©e des URLs entre les workers
        tasks_created = 0
        
        # Calculer √©quitablement la r√©partition des URLs
        base_urls_per_worker = len(urls) // worker_count
        extra_urls = len(urls) % worker_count
        
        start_idx = 0
        for worker_id in range(1, worker_count + 1):
            # Calculer le nombre d'URLs pour ce worker (distribution √©quitable)
            worker_urls_count = base_urls_per_worker + (1 if worker_id <= extra_urls else 0)
            
            # Extraire les URLs pour ce worker
            worker_urls = urls[start_idx:start_idx + worker_urls_count]
            start_idx += worker_urls_count
            
            if worker_urls:
                task_id = str(uuid.uuid4())
                db.download_tasks.insert_one({
                    "_id": task_id,
                    "worker_id": worker_id,
                    "urls": worker_urls,
                    "status": "pending",
                    "created_at": time.time(),
                    "theme": theme,
                    "urls_count": len(worker_urls)
                })
                tasks_created += 1
                flash(f"Worker {worker_id}: {len(worker_urls)} images √† t√©l√©charger (T√¢che: {task_id[:6]}...)", "info")
        
        # Message r√©capitulatif
        summary = (f"{len(urls)} images du th√®me '{theme}' r√©parties entre {tasks_created} worker"
                 f"{'s' if tasks_created > 1 else ''}")
        flash(summary, "success")
        
        return redirect(url_for('main.index'))
    
    # GET request - show the form
    # R√©cup√©rer les th√®mes disponibles
    themes = get_themed_queries().keys()
    
    # V√©rifier si l'auto-scaling est disponible
    auto_scale_available = True  # Toujours disponible maintenant
    current_workers = orchestrator_client.get_current_scale() if orchestrator_client.is_connected() else 1
    
    return render_template('batch_download.html', 
                         themes=themes, 
                         auto_scale_available=auto_scale_available,
                         current_workers=current_workers)

@task_routes.route('/clear_database', methods=['GET', 'POST'])
def clear_database():
    """Supprime TOUTES les images de mani√®re simple et efficace"""
    debug_logger.debug("üîç D√âBUT de clear_database() avec m√©thode %s", request.method)
    
    if request.method == 'POST':
        try:
            debug_logger.debug("üîç SUPPRESSION des collections...")
            # 1. SUPPRIMER TOUTES LES DONN√âES VIA DELETE_MANY (plus s√ªr que drop_collection)
            image_count = db.images.count_documents({})
            tag_count = db.tags.count_documents({})
            like_count = db.user_likes.count_documents({})
            rec_count = db.recommendations.count_documents({})
            
            debug_logger.debug(f"üîç Nombres avant suppression: {image_count} images, {tag_count} tags, {like_count} likes")
            
            # Utiliser delete_many avec un filtre vide pour tout supprimer 
            # (plus s√ªr que drop_collection qui peut causer des probl√®mes d'index)
            db.images.delete_many({})
            db.tags.delete_many({})
            db.user_likes.delete_many({})
            db.recommendations.delete_many({})
            
            # V√©rification apr√®s suppression
            remaining_images = db.images.count_documents({})
            debug_logger.debug(f"üîç Apr√®s suppression: {remaining_images} images restantes")
            
            # 2. ANNULER TOUTES LES T√ÇCHES EN COURS
            deleted_downloads = db.download_tasks.delete_many({"status": {"$in": ["pending", "in_progress"]}}).deleted_count
            deleted_taggings = db.tagging_tasks.delete_many({"status": {"$in": ["pending", "in_progress"]}}).deleted_count
            
            debug_logger.debug(f"üîç T√¢ches supprim√©es: {deleted_downloads} t√©l√©chargements, {deleted_taggings} taggings")
            
            flash(f"‚úÖ Base de donn√©es nettoy√©e: {image_count} images, {tag_count} tags supprim√©s", "success")
            
            # 3. SUPPRIMER LES FICHIERS IMAGES
            try:
                import os
                import glob
                # Nettoyer les fichiers d'image
                image_path = "/data/images"
                debug_logger.debug(f"üîç Tentative de suppression des fichiers dans {image_path}")
                
                if os.path.exists(image_path):
                    files = glob.glob(f"{image_path}/*.jpg") + glob.glob(f"{image_path}/*.jpeg") + glob.glob(f"{image_path}/*.png")
                    debug_logger.debug(f"üîç {len(files)} fichiers trouv√©s")
                    
                    for f in files:
                        try:
                            os.remove(f)
                            debug_logger.debug(f"üîç Supprim√©: {f}")
                        except Exception as file_err:
                            debug_logger.debug(f"üîç Erreur suppression {f}: {file_err}")
                    
                    flash(f"‚úÖ {len(files)} fichiers d'images supprim√©s du disque", "success")
                else:
                    debug_logger.debug(f"üîç Dossier {image_path} non trouv√©")
                    flash(f"‚ö†Ô∏è Dossier d'images non trouv√©", "warning")
            except Exception as e:
                debug_logger.debug(f"üîç Erreur suppression fichiers: {str(e)}")
                flash(f"Erreur lors de la suppression des fichiers: {str(e)}", "warning")
            
            # 4. RED√âMARRER LES WORKERS
            try:
                # Arr√™ter tous les workers
                debug_logger.debug("üîç Tentative d'arr√™t des workers")
                result1 = orchestrator_client.scale_service("image_downloader", 0)
                result2 = orchestrator_client.scale_service("image_tagger", 0)
                
                debug_logger.debug(f"üîç R√©sultat arr√™t: downloader={result1}, tagger={result2}")
                
                # Attendre un peu
                time.sleep(1)
                
                # Red√©marrer un worker minimal
                debug_logger.debug("üîç Red√©marrage d'un worker")
                result3 = orchestrator_client.scale_service("image_downloader", 1)
                debug_logger.debug(f"üîç R√©sultat red√©marrage: {result3}")
                
                flash("‚úÖ Les workers ont √©t√© red√©marr√©s", "success")
            except Exception as e:
                debug_logger.debug(f"üîç Erreur red√©marrage workers: {str(e)}")
                flash(f"Erreur lors du red√©marrage des workers: {str(e)}", "warning")
                
        except Exception as e:
            debug_logger.debug(f"üîç ERREUR GLOBALE: {str(e)}")
            flash(f"‚ùå Erreur lors du nettoyage: {str(e)}", "danger")
        
        debug_logger.debug("üîç FIN clear_database: redirection vers l'accueil")
        return redirect(url_for('main.index'))
    
    # M√©thode GET - rediriger vers l'accueil
    debug_logger.debug("üîç M√©thode GET re√ßue, redirection vers l'accueil")
    return redirect(url_for('main.index'))

@task_routes.route('/debug/<task_id>')
def task_debug(task_id):
    """Page de debug pour une t√¢che sp√©cifique"""
    # Trouver la t√¢che
    task = db.download_tasks.find_one({"_id": task_id})
    task_type = "download"
    
    if not task:
        task = db.tagging_tasks.find_one({"_id": task_id})
        task_type = "tagging"
    
    if not task:
        flash("T√¢che non trouv√©e", "danger")
        return redirect(url_for('worker.worker_debug'))
    
    # R√©cup√©rer des informations sur le worker
    worker_id = task.get("worker_id")
    
    service_name = "image_downloader" if task_type == "download" else "image_tagger"
    
    # Rechercher le conteneur Docker pour ce worker
    container_info = None
    container_logs = None
    container_status = "unknown"
    
    # V√©rifier si Docker est disponible via l'orchestrateur
    try:
        # Obtenir les informations du conteneur via l'orchestrateur client
        container_info = orchestrator_client.get_container_info(service_name, worker_id)
        if container_info:
            container_status = container_info.get("status", "unknown")
            container_logs = orchestrator_client.get_container_logs(service_name, worker_id, 200)
    except Exception as e:
        container_logs = f"Erreur lors de la connexion √† Docker: {e}"
    
    # V√©rifier si le worker existe dans la BDD
    worker_tasks_completed = 0
    if task_type == "download":
        worker_tasks_completed = db.download_tasks.count_documents({"worker_id": worker_id, "status": "completed"})
    else:
        worker_tasks_completed = db.tagging_tasks.count_documents({"worker_id": worker_id, "status": "completed"})
    
    # Obtenir la date de la derni√®re t√¢che compl√©t√©e par ce worker
    last_completed_task = None
    if task_type == "download":
        last_task = db.download_tasks.find_one({"worker_id": worker_id, "status": "completed"}, sort=[("completed_at", -1)])
    else:
        last_task = db.tagging_tasks.find_one({"worker_id": worker_id, "status": "completed"}, sort=[("completed_at", -1)])
    
    last_activity = None
    if last_task and "completed_at" in last_task:
        from datetime import datetime
        last_activity = datetime.fromtimestamp(last_task["completed_at"]).strftime('%Y-%m-%d %H:%M:%S')
    
    # Diagnostic de probl√®mes courants
    diagnostics = []
    
    if container_status != "running":
        diagnostics.append({
            "type": "error",
            "message": f"Le conteneur du worker n'est pas en cours d'ex√©cution (statut: {container_status})",
            "solution": "Red√©marrez le conteneur ou cr√©ez-en un nouveau pour ce worker"
        })
    
    if worker_tasks_completed == 0:
        diagnostics.append({
            "type": "warning",
            "message": "Ce worker n'a jamais compl√©t√© de t√¢che",
            "solution": "V√©rifiez que le worker est correctement configur√©"
        })
    
    if last_activity is None:
        diagnostics.append({
            "type": "warning",
            "message": "Aucune activit√© r√©cente d√©tect√©e pour ce worker",
            "solution": "V√©rifiez que le worker est op√©rationnel et peut acc√©der √† la base de donn√©es"
        })
    
    # V√©rifier si la t√¢che est bien assign√©e √† un worker existant
    if container_info is None:
        diagnostics.append({
            "type": "error",
            "message": f"Aucun conteneur trouv√© pour le worker {worker_id}",
            "solution": "R√©assignez cette t√¢che √† un worker actif ou cr√©ez un nouveau worker avec cet ID"
        })
    
    # Actions correctives possibles
    actions = [
        {
            "id": "redistribute",
            "name": "R√©assigner la t√¢che",
            "description": "R√©assigne cette t√¢che √† un autre worker actif",
            "action": f"/tasks/redistribute/{task_id}"
        },
        {
            "id": "restart_container",
            "name": "Red√©marrer le conteneur",
            "description": "Red√©marre le conteneur associ√© √† ce worker",
            "action": f"/workers/restart/{service_name}/{worker_id}"
        },
        {
            "id": "create_container",
            "name": "Cr√©er un nouveau conteneur",
            "description": "Cr√©e un nouveau conteneur pour ce worker",
            "action": f"/workers/create/{service_name}/{worker_id}"
        }
    ]
    
    return render_template('task_debug.html', 
                          task=task,
                          task_type=task_type,
                          worker_id=worker_id,
                          worker_tasks_completed=worker_tasks_completed,
                          last_activity=last_activity,
                          container_info=container_info,
                          container_logs=container_logs,
                          diagnostics=diagnostics,
                          actions=actions)

@task_routes.route('/redistribute/<task_id>', methods=['POST'])
def redistribute_task(task_id):
    """R√©assigne une t√¢che √† un autre worker via l'orchestrateur"""
    # Si nous sommes en mode fallback, impl√©menter un m√©canisme direct de redistribution
    if orchestrator_client.fallback_mode:
        try:
            # D√©terminer le type de t√¢che
            task = db.download_tasks.find_one({"_id": task_id})
            collection_name = "download_tasks"
            service_name = "image_downloader"
            
            if not task:
                task = db.tagging_tasks.find_one({"_id": task_id})
                collection_name = "tagging_tasks"
                service_name = "image_tagger"
                
            if not task:
                flash("T√¢che non trouv√©e", "danger")
                return redirect(url_for('task.task_debug', task_id=task_id))
            
            # R√©cup√©rer l'ID worker actuel
            current_worker_id = task.get("worker_id", 1)
            
            # Choisir un worker diff√©rent (simplement alterner entre 1 et 2)
            new_worker_id = 2 if current_worker_id == 1 else 1
            
            # Mettre √† jour la t√¢che avec le nouveau worker_id
            db[collection_name].update_one(
                {"_id": task_id},
                {"$set": {"worker_id": new_worker_id}}
            )
            
            flash(f"T√¢che r√©assign√©e au worker {new_worker_id} (mode direct)", "success")
            return redirect(url_for('task.task_debug', task_id=task_id))
        except Exception as e:
            flash(f"Erreur lors de la r√©assignation de la t√¢che: {str(e)}", "danger")
            return redirect(url_for('task.task_debug', task_id=task_id))
    
    # Utiliser l'orchestrateur si disponible
    success, message = orchestrator_client.redistribute_task(task_id)
    
    if success:
        flash(message, "success")
    else:
        flash(f"Erreur: {message}", "danger")
    
    return redirect(url_for('task.task_debug', task_id=task_id))