import os
import time
import logging
import docker
from pymongo import MongoClient
import traceback
import json
import subprocess

# Configuration du logger avec plus de d√©tails
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levellevel)s - %(message)s',
                   handlers=[
                       logging.StreamHandler(),  # Pour afficher dans la console
                   ])
logger = logging.getLogger('autoscaler')

# Configuration MongoDB
DB_URL = os.environ.get('DATABASE_URL', 'mongodb://database:27017/imagesdb')
client = MongoClient(DB_URL)
db = client.get_database()

# Constantes pour l'autoscaling
IMAGES_PER_WORKER = 20  # Maximum d'images par worker
MAX_WORKERS = 5  # Nombre maximum de workers par type
DEPLOYMENT_TIMEOUT = 60  # Timeout en secondes pour le d√©ploiement d'un conteneur
PROJECT_NAME = "partie2"  # Nom du projet docker-compose

# Initialisation du client Docker avec gestion des erreurs explicite
try:
    docker_client = docker.from_env()
    # Test de la connexion
    docker_client.ping()
    logger.info("‚úÖ Connexion au d√©mon Docker √©tablie avec succ√®s")
except Exception as e:
    logger.error(f"‚ùå ERREUR lors de la connexion au d√©mon Docker: {e}")
    traceback.print_exc()
    docker_client = None

def get_active_worker_ids(service_name):
    """R√©cup√®re les IDs de worker actuellement actifs pour un service"""
    worker_ids = []
    
    try:
        if not docker_client:
            return worker_ids
        
        containers = docker_client.containers.list(
            all=True,
            filters={
                "label": [f"com.docker.compose.service={service_name}"]
            }
        )
        
        for container in containers:
            # R√©cup√©rer le worker_id des variables d'environnement
            env_vars = container.attrs.get('Config', {}).get('Env', [])
            for env in env_vars:
                if env.startswith('WORKER_ID='):
                    worker_id = int(env.split('=')[1])
                    if container.status == 'running':
                        worker_ids.append(worker_id)
                    break
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des IDs de worker actifs: {e}")
    
    return worker_ids

def get_next_worker_id(service_name):
    """D√©termine le prochain worker_id disponible"""
    active_ids = get_active_worker_ids(service_name)
    
    if not active_ids:
        return 1  # Si aucun worker actif, commencer √† 1
    
    # Trouver le premier ID disponible
    next_id = 1
    while next_id in active_ids and next_id <= MAX_WORKERS:
        next_id += 1
    
    return next_id

def create_new_worker_container(service_name, worker_id):
    """Cr√©e un nouveau conteneur avec un worker_id sp√©cifique"""
    try:
        logger.info(f"üöÄ Cr√©ation d'un nouveau conteneur {service_name} avec worker_id={worker_id}")
        
        # Nom du service et du conteneur
        container_name = f"{service_name}_{worker_id}"
        
        # V√©rifie si un conteneur avec ce nom existe d√©j√† et le supprime
        try:
            existing = docker_client.containers.get(container_name)
            logger.warning(f"‚ö†Ô∏è Un conteneur {container_name existe d√©j√†, statut: {existing.status}")
            
            if existing.status != 'running':
                logger.warning(f"‚ö†Ô∏è Suppression du conteneur existant {container_name}")
                existing.remove(force=True)
                logger.info(f"‚úÖ Conteneur {container_name} supprim√©")
        except docker.errors.NotFound:
            pass  # Le conteneur n'existe pas, c'est ce qu'on veut
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la v√©rification/suppression du conteneur existant: {e}")
        
        # Cr√©er le nouveau conteneur avec le bon worker_id
        image_name = f"{COMPOSE_PROJECT}_{service_name}"
        logger.info(f"üîÑ Utilisation de l'image {image_name}")
        
        new_container = docker_client.containers.run(
            image_name,
            name=container_name,
            detach=True,
            environment={
                "WORKER_ID": str(worker_id),
                "DATABASE_URL": DB_URL
            },
            volumes={
                f"{COMPOSE_PROJECT}_shared_data": {"bind": "/data", "mode": "rw"}
            },
            network=f"{COMPOSE_PROJECT}_app_network",
            labels={
                "com.docker.compose.project": COMPOSE_PROJECT,
                "com.docker.compose.service": service_name
            }
        )
        
        logger.info(f"‚úÖ Conteneur {container_name} cr√©√© avec succ√®s, ID: {new_container.id[:12]}")
        
        # V√©rifier le statut du nouveau conteneur
        status = new_container.status
        logger.info(f"üìä Statut initial du conteneur {container_name}: {status}")
        
        # Attendre que le conteneur soit pr√™t
        start_time = time.time()
        while time.time() - start_time < DEPLOYMENT_TIMEOUT:
            new_container.reload()
            status = new_container.status
            logger.info(f"‚è≥ Attente du d√©marrage du conteneur {container_name}, statut: {status}")
            
            if status == 'running':
                logger.info(f"‚úÖ Conteneur {container_name} d√©marr√© avec succ√®s")
                # V√©rifier les logs du conteneur pour s'assurer qu'il fonctionne
                logs = new_container.logs().decode('utf-8')
                logger.info(f"üìú Derniers logs du conteneur {container_name}: {logs[-200:] if logs else 'Pas de logs'}")
                return True
            
            time.sleep(2)
        
        logger.error(f"‚ùå Timeout: Le conteneur {container_name} n'est pas pass√© √† l'√©tat 'running'")
        return False
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la cr√©ation du conteneur {service_name}_{worker_id}: {e}")
        traceback.print_exc()
        return False

def scale_service_compose(service_name, count):
    """Utilise docker-compose pour mettre √† l'√©chelle un service"""
    try:
        # S'assurer que le nombre est dans les limites
        count = max(1, min(count, MAX_WORKERS))
        
        logger.info(f"üîÑ Tentative de mise √† l'√©chelle de {service_name} √† {count} instances via docker-compose")
        
        # Ex√©cuter la commande docker-compose scale
        cmd = f"docker-compose -p {COMPOSE_PROJECT} -f {COMPOSE_FILE} up -d --scale {service_name}={count} {service_name}"
        logger.info(f"üìù Commande: {cmd}")
        
        process = subprocess.run(
            cmd,
            shell=True,
            check=False,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        
        if process.returncode == 0:
            logger.info(f"‚úÖ Scaling r√©ussi pour {service_name}: {count} instances")
            stdout = process.stdout.decode('utf-8')
            logger.info(f"üìù Sortie standard: {stdout}")
            return True
        else:
            stderr = process.stderr.decode('utf-8')
            logger.error(f"‚ùå √âchec du scaling pour {service_name}: {stderr}")
            
            # Le scaling via docker-compose a √©chou√©, tentons une approche manuelle
            logger.info(f"üîÑ Tentative de scaling manuel pour {service_name}")
            return scale_service_manually(service_name, count)
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scaling via docker-compose: {e}")
        traceback.print_exc()
        
        # Fallback sur la m√©thode manuelle
        return scale_service_manually(service_name, count)

def scale_service_manually(service_name, target_count):
    """Tente de scaler manuellement un service en cr√©ant/supprimant des conteneurs"""
    try:
        # Obtenir les worker IDs actuels
        active_worker_ids = get_active_worker_ids(service_name)
        current_count = len(active_worker_ids)
        
        logger.info(f"üìä √âtat actuel de {service_name}: {current_count} workers actifs avec IDs {active_worker_ids}")
        
        # Si nous avons besoin de plus de workers
        if target_count > current_count:
            success_count = 0
            for _ in range(target_count - current_count):
                next_id = get_next_worker_id(service_name)
                if next_id <= MAX_WORKERS:
                    if create_new_worker_container(service_name, next_id):
                        success_count += 1
                else:
                    logger.warning(f"‚ö†Ô∏è Impossible de cr√©er plus de workers, limite atteinte: {MAX_WORKERS}")
                    break
            
            logger.info(f"üìä Scaling manuel termin√©: {success_count}/{target_count - current_count} nouvelles instances cr√©√©es")
            return success_count > 0
        
        # Si nous avons besoin de moins de workers (ne pas impl√©menter pour l'instant)
        elif target_count < current_count:
            logger.info(f"‚ö†Ô∏è R√©duction du nombre de workers non impl√©ment√©e pour le moment")
            return False
        
        # D√©j√† au bon nombre
        else:
            logger.info(f"‚úÖ D√©j√† au bon nombre de workers ({current_count})")
            return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scaling manuel: {e}")
        traceback.print_exc()
        return False

def check_and_scale():
    """V√©rifie la charge actuelle et scale les services si n√©cessaire"""
    try:
        # R√©cup√©rer les informations de tous les conteneurs pour le monitoring
        all_containers = []
        if docker_client:
            containers = docker_client.containers.list(all=True)
            for container in containers:
                all_containers.append({
                    "id": container.id[:12],
                    "name": container.name,
                    "status": container.status,
                    "labels": container.labels
                })
        
        # Enregistrer l'√©tat actuel pour le monitoring
        db.container_status.delete_many({})
        for container in all_containers:
            db.container_status.insert_one(container)
        
        # === DOWNLOADER SCALING ===
        # Compter les URLs en attente de t√©l√©chargement
        pending_download_urls = 0
        download_tasks = list(db.download_tasks.find({"status": "pending"}))
        
        for task in download_tasks:
            urls = task.get("urls", [])
            if isinstance(urls, list):
                pending_download_urls += len(urls)
        
        # Calculer le nombre de workers n√©cessaires
        required_downloaders = max(1, min(MAX_WORKERS, (pending_download_urls + IMAGES_PER_WORKER - 1) // IMAGES_PER_WORKER))
        logger.info(f"üìà URLs en attente: {pending_download_urls}, t√©l√©chargeurs n√©cessaires: {required_downloaders}")
        
        # V√©rifier l'√©tat actuel des downloaders
        active_downloader_ids = get_active_worker_ids("image_downloader")
        current_downloaders = len(active_downloader_ids)
        
        # Ajuster le nombre de downloaders si n√©cessaire et s'il y a des t√¢ches en attente
        if pending_download_urls > 0 and required_downloaders > current_downloaders:
            logger.info(f"üîÑ Besoin de {required_downloaders} t√©l√©chargeurs, actuellement {current_downloaders}")
            
            # Essayer d'abord l'approche docker-compose
            success = scale_service_compose("image_downloader", required_downloaders)
            
            if not success:
                # Fallback: cr√©er manuellement les conteneurs suppl√©mentaires
                for i in range(current_downloaders + 1, required_downloaders + 1):
                    worker_id = get_next_worker_id("image_downloader")
                    create_new_worker_container("image_downloader", worker_id)
        
        # === TAGGER SCALING ===
        # Compter les images non tagu√©es
        pending_tag_images = db.images.count_documents({"tagged": {"$ne": True}})
        
        # Calculer le nombre de taggers n√©cessaires
        required_taggers = max(1, min(MAX_WORKERS, (pending_tag_images + IMAGES_PER_WORKER - 1) // IMAGES_PER_WORKER))
        logger.info(f"üìà Images √† tagger: {pending_tag_images, taggers n√©cessaires: {required_taggers}")
        
        # V√©rifier l'√©tat actuel des taggers
        active_tagger_ids = get_active_worker_ids("image_tagger")
        current_taggers = len(active_tagger_ids)
        
        # Ajuster le nombre de taggers si n√©cessaire et s'il y a des images √† tagger
        if pending_tag_images > 0 and required_taggers > current_taggers:
            logger.info(f"üîÑ Besoin de {required_taggers} taggers, actuellement {current_taggers}")
            
            # Essayer d'abord l'approche docker-compose
            success = scale_service_compose("image_tagger", required_taggers)
            
            if not success:
                # Fallback: cr√©er manuellement les conteneurs suppl√©mentaires
                for i in range(current_taggers + 1, required_taggers + 1):
                    worker_id = get_next_worker_id("image_tagger")
                    create_new_worker_container("image_tagger", worker_id)
        
        # Assurer que les t√¢ches sont bien r√©parties entre les workers
        redistribute_pending_tasks()
        
        # Enregistrer les statistiques pour le dashboard
        db.autoscaler_stats.insert_one({
            "timestamp": time.time(),
            "pending_download_urls": pending_download_urls,
            "required_downloaders": required_downloaders,
            "current_downloaders": current_downloaders,
            "pending_tag_images": pending_tag_images,
            "required_taggers": required_taggers,
            "current_taggers": current_taggers,
            "container_count": len(all_containers)
        })
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur dans la fonction check_and_scale: {e}")
        traceback.print_exc()
        return False

def redistribute_pending_tasks():
    """Redistribue les t√¢ches en attente aux workers actifs"""
    try:
        # R√©cup√©rer les IDs des workers actifs
        downloader_ids = get_active_worker_ids("image_downloader")
        tagger_ids = get_active_worker_ids("image_tagger")
        
        # Si aucun worker n'est actif, inutile de continuer
        if not downloader_ids and not tagger_ids:
            logger.warning("‚ö†Ô∏è Aucun worker actif, impossible de redistribuer les t√¢ches")
            return False
        
        # Redistribuer les t√¢ches de t√©l√©chargement
        if downloader_ids:
            pending_download_tasks = list(db.download_tasks.find({"status": "pending"}))
            
            for i, task in enumerate(pending_download_tasks):
                # Assigner la t√¢che √† un worker actif de mani√®re cyclique
                worker_id = downloader_ids[i % len(downloader_ids)]
                
                # Mettre √† jour la t√¢che uniquement si n√©cessaire
                current_worker_id = task.get("worker_id")
                if current_worker_id != worker_id or current_worker_id not in downloader_ids:
                    db.download_tasks.update_one(
                        {"_id": task["_id"]},
                        {"$set": {"worker_id": worker_id}}
                    )
                    logger.info(f"‚ôªÔ∏è T√¢che de t√©l√©chargement {task['_id']} r√©attribu√©e au worker {worker_id}")
        
        # Redistribuer les t√¢ches de tagging
        if tagger_ids:
            pending_tagging_tasks = list(db.tagging_tasks.find({"status": "pending"}))
            
            for i, task in enumerate(pending_tagging_tasks):
                # Assigner la t√¢che √† un worker actif de mani√®re cyclique
                worker_id = tagger_ids[i % len(tagger_ids)]
                
                # Mettre √† jour la t√¢che uniquement si n√©cessaire
                current_worker_id = task.get("worker_id")
                if current_worker_id != worker_id or current_worker_id not in tagger_ids:
                    db.tagging_tasks.update_one(
                        {"_id": task["_id"]},
                        {"$set": {"worker_id": worker_id}}
                    )
                    logger.info(f"‚ôªÔ∏è T√¢che de tagging {task['_id']} r√©attribu√©e au worker {worker_id}")
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la redistribution des t√¢ches: {e}")
        traceback.print_exc()
        return False

def run_autoscaler():
    """Fonction principale qui v√©rifie p√©riodiquement la charge et ajuste les workers"""
    logger.info("üöÄ D√©marrage de l'autoscaler")
    
    while True:
        try:
            check_and_scale()
            time.sleep(10)  # V√©rifier toutes les 10 secondes
        except Exception as e:
            logger.error(f"‚ùå Erreur dans la boucle autoscaler: {e}")
            traceback.print_exc()
            time.sleep(30)  # Attendre plus longtemps en cas d'erreur

if __name__ == "__main__":
    run_autoscaler()
